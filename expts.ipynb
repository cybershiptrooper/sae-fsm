{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7074f7766f20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from sae_lens import SAE\n",
    "import transformer_lens as tl\n",
    "\n",
    "model = tl.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n    <title>My Website</title>\\n</head>\\n<body>\\n    <h>Welcome to My Website</h1>\\n    <p>This is a paragraph.</p>\\n    <div class=\"content\">\\n        <h2>Here is a heading</h2>\\n        <p>This is another paragraph.</p>\\n    </div>\\n</html>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_prompt = \"\"\"<html>\n",
    "<head>\n",
    "    <title>My Website</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h>Welcome to My Website</h1>\n",
    "    <p>This is a paragraph.</p>\n",
    "    <div class=\"content\">\n",
    "        <h2>Here is a heading</h2>\n",
    "        <p>This is another paragraph.</p>\n",
    "    </div>\n",
    "</html>\n",
    "\"\"\"\n",
    "html_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['gpt2-small-res-jb', 'gpt2-small-hook-z-kk', 'gpt2-small-mlp-tm', 'gpt2-small-res-jb-feature-splitting', 'gpt2-small-resid-post-v5-32k', 'gpt2-small-resid-post-v5-128k', 'gemma-2b-res-jb', 'gemma-2b-it-res-jb', 'mistral-7b-res-wg', 'gpt2-small-resid-mid-v5-32k', 'gpt2-small-resid-mid-v5-128k', 'gpt2-small-mlp-out-v5-32k', 'gpt2-small-mlp-out-v5-128k', 'gpt2-small-attn-out-v5-32k', 'gpt2-small-attn-out-v5-128k', 'gemma-scope-2b-pt-res', 'gemma-scope-2b-pt-res-canonical', 'gemma-scope-2b-pt-mlp', 'gemma-scope-2b-pt-mlp-canonical', 'gemma-scope-2b-pt-att', 'gemma-scope-2b-pt-att-canonical', 'gemma-scope-9b-pt-res', 'gemma-scope-9b-pt-res-canonical', 'gemma-scope-9b-pt-att', 'gemma-scope-9b-pt-att-canonical', 'gemma-scope-9b-pt-mlp', 'gemma-scope-9b-pt-mlp-canonical', 'gemma-scope-9b-it-res', 'gemma-scope-9b-it-res-canonical', 'gemma-scope-27b-pt-res', 'gemma-scope-27b-pt-res-canonical', 'pythia-70m-deduped-res-sm', 'pythia-70m-deduped-mlp-sm', 'pythia-70m-deduped-att-sm', 'gpt2-small-res_sll-ajt', 'gpt2-small-res_slefr-ajt', 'gpt2-small-res_scl-ajt', 'gpt2-small-res_sle-ajt', 'gpt2-small-res_sce-ajt', 'gpt2-small-res_scefr-ajt', 'sae_bench_gemma-2-2b_sweep_standard_ctx128_ef2_0824', 'sae_bench_gemma-2-2b_sweep_standard_ctx128_ef8_0824', 'sae_bench_gemma-2-2b_sweep_topk_ctx128_ef2_0824', 'sae_bench_gemma-2-2b_sweep_topk_ctx128_ef8_0824', 'sae_bench_pythia70m_sweep_gated_ctx128_0730', 'sae_bench_pythia70m_sweep_panneal_ctx128_0730', 'sae_bench_pythia70m_sweep_standard_ctx128_0712', 'sae_bench_pythia70m_sweep_topk_ctx128_0730', 'llama-3-8b-it-res-jh'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sae_lens/sae.py:136: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import sae_lens\n",
    "\n",
    "with open(\"/usr/local/lib/python3.10/dist-packages/sae_lens/pretrained_saes.yaml\", \"r\") as file:\n",
    "    pretrained_saes = yaml.safe_load(file)\n",
    "print(pretrained_saes.keys())\n",
    "\n",
    "\n",
    "RELEASE = \"gpt2-small-res-jb\"\n",
    "\n",
    "saes = {}\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    saes[layer] = sae_lens.SAE.from_pretrained(\n",
    "        release=RELEASE,\n",
    "        sae_id=f\"blocks.{layer}.hook_resid_pre\",\n",
    "        device=\"cuda\",\n",
    "    )[0].to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'html', '>', '\\n', '<', 'head', '>', '\\n', ' ', ' ', ' ', ' <', 'title', '>', 'My', ' Website', '</', 'title', '>', '\\n', '</', 'head', '>', '\\n', '<', 'body', '>', '\\n', ' ', ' ', ' ', ' <', 'h', '>', 'Welcome', ' to', ' My', ' Website', '</', 'h', '1', '>', '\\n', ' ', ' ', ' ', ' <', 'p', '>', 'This', ' is', ' a', ' paragraph', '.</', 'p', '>', '\\n', ' ', ' ', ' ', ' <', 'div', ' class', '=\"', 'content', '\">', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' <', 'h', '2', '>', 'Here', ' is', ' a', ' heading', '</', 'h', '2', '>', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' <', 'p', '>', 'This', ' is', ' another', ' paragraph', '.</', 'p', '>', '\\n', ' ', ' ', ' ', ' </', 'div', '>', '\\n', '</', 'html', '>', '\\n']\n"
     ]
    }
   ],
   "source": [
    "html_tokens = model.tokenizer(html_prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "print(model.to_str_tokens(html_tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import (\n",
    "    HookedTransformerConfig,\n",
    "    HookedTransformer,\n",
    "    FactoredMatrix,\n",
    "    ActivationCache,\n",
    ")\n",
    "\n",
    "def get_cache_fwd_and_bwd(model, tokens, metric, layers):\n",
    "    model.reset_hooks()\n",
    "    cache = {}\n",
    "\n",
    "    def forward_cache_hook(act, hook):\n",
    "        cache[hook.name] = act.detach()\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add_hook(f\"blocks.{layer}.hook_resid_post\", forward_cache_hook, \"fwd\")\n",
    "\n",
    "    grad_cache = {}\n",
    "\n",
    "    def backward_cache_hook(act, hook):\n",
    "        grad_cache[hook.name] = act.detach()\n",
    "\n",
    "    for layer in layers:\n",
    "        model.add_hook(f\"blocks.{layer}.hook_resid_post\", backward_cache_hook, \"bwd\")\n",
    "    torch.set_grad_enabled(True)\n",
    "    logits = model(tokens.clone())\n",
    "    value = metric(logits)\n",
    "    value.backward()\n",
    "    torch.set_grad_enabled(False)\n",
    "    model.reset_hooks()\n",
    "    return (\n",
    "        value.item(),\n",
    "        ActivationCache(cache, model),\n",
    "        ActivationCache(grad_cache, model),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 116, 50257])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(html_tokens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(logits):\n",
    "    \"\"\"Next token prediction\"\"\"\n",
    "    # cross entropy loss for next token prediction\n",
    "    ce_loss = torch.nn.CrossEntropyLoss()\n",
    "    loss = ce_loss(logits.view(-1, logits.size(-1)), html_tokens.view(-1))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.581077575683594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, fwd_cache, bwd_cache = get_cache_fwd_and_bwd(\n",
    "    model, html_tokens, metric, list(range(model.cfg.n_layers))\n",
    ")\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.581077575683594"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nutils.show_df(nutils.create_vocab_df(logits[0, 55]).head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
